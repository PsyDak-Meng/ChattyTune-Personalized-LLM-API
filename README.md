# Project Title: GPT-neo-x 20B Fine-tuning & RAG Deployment

## Overview
This project focuses on fine-tuning the GPT-neo-x model with 20B parameters using the arxiv-cs-ml Hugging Face dataset. Additionally, it implements QLoRA to significantly reduce the number of trainable parameters to 0.08%, enabling faster training within a Colab environment. Furthermore, RAG (Retrieval-Augmented Generation) with cosine-similarity retrieval has been deployed to optimize model outputs and minimize hallucination.

## Features
- Fine-tuned GPT-neo-x model with 20 billion parameters.
- Utilized arxiv-cs-ml Hugging Face dataset for fine-tuning.
- Implemented QLoRA to reduce trainable parameters to 0.08%, facilitating faster training.
- Deployed RAG with cosine-similarity retrieval to enhance output optimization and reduce hallucination.

## Installation
1. Clone the repository
